{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch as t \n",
    "from datasets import load_dataset\n",
    "import transformer_lens.utils as utils\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import make_token_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = LanguageModel('EleutherAI/pythia-410m', device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-410m', device_map='auto')\n",
    "config = AutoConfig.from_pretrained('EleutherAI/pythia-410m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e81421e5ce4434a83ac47cc255c4eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/961 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37532fa61a04520a025d73dac27011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c4-en-10k.py:   0%|          | 0.00/2.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7093dd82ef2b42048f64f05d69ee5ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/13.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c405e68166e24ee5aa331d644387cbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ad89cece144058aa273cfcec35d0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "t.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "nnmodel.eval()\n",
    "\n",
    "#data = load_dataset(\"stas/openwebtext-10k\", split='train')\n",
    "data = load_dataset(\"stas/c4-en-10k\", split='train')\n",
    "first_1k = data.select(range(1000))\n",
    "\n",
    "tokenized_data = utils.tokenize_and_concatenate(first_1k, tokenizer, max_length=256, column_name='text')\n",
    "\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "token_df = make_token_df(tokenized_data['tokens'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_neuron_layer = config.num_hidden_layers - 1\n",
    "\n",
    "all_neuron_indices = list(range(0, 4* config.hidden_size))\n",
    "all_neurons = [f\"{entropy_neuron_layer}.{i}\" for i in all_neuron_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "from tools.decoding import decode_next_token_with_sampling\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generation(nnmodel, tokenizer, prompt, n_new_tokens=10, use_sampling=False):\n",
    "    device = nnmodel.device\n",
    "    # Encode the prompt\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, padding=True, padding_side=\"left\").to(device)\n",
    "    prompt_tokens = model_input[\"input_ids\"].to(device)\n",
    "    batch_size = prompt_tokens.shape[0]\n",
    "    prompt_len = prompt_tokens.shape[1] - 1\n",
    "\n",
    "    # Container for final results\n",
    "    probas_list = []\n",
    "    l2toks = {}\n",
    "\n",
    "    with t.no_grad():\n",
    "        model_input_base = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, padding=True, padding_side=\"left\").to(device)\n",
    "        start_len = model_input_base[\"input_ids\"].shape[1]\n",
    "        \n",
    "        probas_for_this_layer = []\n",
    "\n",
    "        model_input = {key: value.clone().detach() for key, value in model_input_base.items()}\n",
    "\n",
    "        for _ in range(n_new_tokens): \n",
    "\n",
    "            # Run the forward pass under the patch\n",
    "            with nnmodel.trace(model_input, validate=False, scan=False):\n",
    "\n",
    "                logits = nnmodel.lm_head.output[:, -1, :].save()\n",
    "            \n",
    "            if use_sampling:\n",
    "                # Remove .value since logits is already a tensor\n",
    "                logits_value = logits.cpu()\n",
    "                probs, next_token = decode_next_token_with_sampling(logits_value)\n",
    "                probas_for_this_layer.append(probs.cpu())\n",
    "                next_token = next_token.to(device)\n",
    "            else:\n",
    "                probs = F.softmax(logits.value, dim=-1)  # shape (batch_size, vocab_size)\n",
    "                probas_for_this_layer.append(probs.cpu())\n",
    "\n",
    "                # Pick the next token (greedy). Could do top-k, sampling, etc.\n",
    "                next_token = t.argmax(logits.value, dim=-1, keepdim=True)  # shape (batch_size, 1)\n",
    "\n",
    "            # Append the new token to 'toks'\n",
    "            #toks = t.cat([toks, next_token.to(device)], dim=-1)\n",
    "            model_input[\"input_ids\"] = t.cat([model_input[\"input_ids\"], next_token], dim=-1)\n",
    "            model_input[\"attention_mask\"] = t.cat([model_input[\"attention_mask\"], t.ones_like(next_token)], dim=-1)\n",
    "            if t.all(next_token == tokenizer.eos_token_id):\n",
    "                break\n",
    "        # Stack probabilities: shape => (batch_size, n_new_tokens, vocab_size)\n",
    "        probas_for_this_layer = t.stack(probas_for_this_layer, dim=1)\n",
    "        probas_list.append(probas_for_this_layer)\n",
    "\n",
    "        # Save final generated tokens (just the newly generated portion)\n",
    "        # shape => (batch_size, n_new_tokens)\n",
    "        newly_generated_tokens = model_input[\"input_ids\"][:, start_len:].detach().cpu()\n",
    "        # Convert each row in the batch to text\n",
    "        decoded_texts = [tokenizer.decode(seq) for seq in newly_generated_tokens]\n",
    "    \n",
    "    # Now probas_list has length == number of tested layers\n",
    "    # Each element is shape (batch_size, n_new_tokens, vocab_size)\n",
    "    # Stack them => (n_layers_tested, batch_size, n_new_tokens, vocab_size)\n",
    "    probas = t.stack(probas_list, dim=0)\n",
    "\n",
    "    return l2toks, probas\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
